list3 = append(list3,mis1)
list4 = append(list4,mis2)
}
boxplot(list1,list2,names=c('GLM','gamsel'),main = 'Misclassification of Nonspam Messages')
boxplot(list3,list4,names=c('GLM','gamsel'),main = 'Misclassification')
list1 = c()
list2 = c()
list3 = c()
list4 = c()
for (i in 1:10){
transfSpam <- spam
transfSpam[,-58] <- log(transfSpam[,-58] + 1)
nTest <- 1000
indsTest <- sample(1:nrow(transfSpam),nTest,
replace = FALSE)
indsTrain <- setdiff(1:nrow(transfSpam),indsTest)
transfSpamTest <- transfSpam[indsTest,]
transfSpamTrain <- transfSpam[indsTrain,]
fit = glm(type~our+over+remove+internet+free+business+credit+your+font+num000+money+hp +
hpl+george+data+num1999+meeting+project+re+edu+
conference+charSemicolon+charExclamation+charDollar+
capitalAve+capitalLong+capitalTotal,
family = binomial,data = transfSpamTrain)
pred_prob = predict(fit,transfSpamTest, type="response")
pred = rep("nonspam", dim(transfSpamTest)[1])
pred[pred_prob > .5] = "spam"
#confusion matrix
conf1 = table(pred, transfSpamTest$type)
pred_prob = predict.gamsel(cvmodel$gamsel.fit, transfSpamTest[,-58], index=49, type="response")
pred = rep("nonspam", dim(transfSpamTest)[1])
pred[pred_prob> .5] = "spam"
#confusion matrix
conf2 = table(pred, transfSpamTest$type)
error1 = conf1[2,1]/(conf1[1,1]+conf1[2,1])
error2 = conf2[2,1]/(conf2[1,1]+conf2[2,1])
list1 = append(list1,error1)
list2 = append(list2,error2)
mis1 = (conf1[1,2]+conf1[2,1])/1000
mis2 = (conf2[1,2]+conf2[2,1])/1000
list3 = append(list3,mis1)
list4 = append(list4,mis2)
}
boxplot(list1,list2,names=c('GLM','gamsel'),main = 'Misclassification of Nonspam Messages')
boxplot(list3,list4,names=c('GLM','gamsel'),main = 'Misclassification')
par(mfrow=c(2,1))
boxplot(list1,list2,names=c('GLM','gamsel'),main = 'Misclassification of Nonspam Messages')
boxplot(list3,list4,names=c('GLM','gamsel'),main = 'Misclassification')
par(mfrow=c(1,2))
boxplot(list1,list2,names=c('GLM','gamsel'),main = 'Misclassification of Nonspam Messages')
boxplot(list3,list4,names=c('GLM','gamsel'),main = 'Misclassification')
library('astsa')
install.packages('astsa')
##2.2
library('astsa')
##2.2
# Add P(t-4) to regression
temp  = tempr-mean(tempr)  # center temperature
temp2 = temp^2             # square it
trend = time(cmort)        # time
part_lag4 = lag(part, 4)
part_lag4
lag(part,-4)
temp  = tempr-mean(tempr)  # center temperature
temp2 = temp^2             # square it
trend = time(cmort)        # time
part_lag4 = lag(part, 4)
data_set = ts.intersect(cmort, temp, temp2, trend, part, part_lag4, dframe = TRUE)
fit = lm(cmort~ trend + temp + temp2 + part + part_lag4, data = data_set, na.action=NULL)
summary(fit)
library('astsa')
##2.2
# Add P(t-4) to regression
temp  = tempr-mean(tempr)  # center temperature
temp2 = temp^2             # square it
trend = time(cmort)        # time
part_lag4 = lag(part, -4)
data_set = ts.intersect(cmort, temp, temp2, trend, part, part_lag4, dframe = TRUE)
fit = lm(cmort~ trend + temp + temp2 + part + part_lag4, data = data_set, na.action=NULL)
summary(fit)
library('astsa')
##2.2
# Add P(t-4) to regression
temp  = tempr-mean(tempr)  # center temperature
temp2 = temp^2             # square it
trend = time(cmort)        # time
part_lag4 = lag(part, -4)
data_set = ts.intersect(cmort, temp, temp2, trend, part, part_lag4, dframe = TRUE)
fit = lm(cmort~ trend + temp + temp2 + part + part_lag4, data = data_set, na.action=NULL)
summary(fit)
library('astsa')
##2.2
# Add P(t-4) to regression
temp  = tempr-mean(tempr)  # center temperature
temp2 = temp^2             # square it
trend = time(cmort)        # time
part_lag4 = lag(part, 44)
data_set = ts.intersect(cmort, temp, temp2, trend, part, part_lag4, dframe = TRUE)
fit = lm(cmort~ trend + temp + temp2 + part + part_lag4, data = data_set, na.action=NULL)
summary(fit)
library('astsa')
##2.2
# Add P(t-4) to regression
temp  = tempr-mean(tempr)  # center temperature
temp2 = temp^2             # square it
trend = time(cmort)        # time
part_lag4 = lag(part, 44)
data_set = ts.intersect(cmort, temp, temp2, trend, part, part_lag4)
fit = lm(cmort~ trend + temp + temp2 + part + part_lag4, data = data_set, na.action=NULL)
summary(fit)
ar = arima.sim(list(order=c(1,0,0), ar=.6), n=100)
ma = arima.sim(list(order=c(0,0,1), ma=.9), n=100)
arma = arima.sim(list(order=c(1,0,1), ar=.6, ma=.9), n=100) par(mfcol=c(1,2))
acf(ar)
pacf(ar)
par(mfcol=c(1,2))
acf(ma)
pacf(ma)
par(mfcol=c(1,2))
acf(arma)
pacf(arma)
ar = arima.sim(list(order=c(1,0,0), ar=.6), n=100)
ma = arima.sim(list(order=c(0,0,1), ma=.9), n=100)
arma = arima.sim(list(order=c(1,0,1), ar=.6, ma=.9), n=100)
par(mfcol=c(1,2))
acf(ar)
pacf(ar)
par(mfcol=c(1,2))
acf(ma)
pacf(ma)
par(mfcol=c(1,2))
acf(arma)
pacf(arma)
install.packages('car')
data = read.csv('step1_data.csv')
library(car)
data = read.csv('step1_data.csv')
3.1*100.65%
3.1*100.65、100
3.1*100.65/100
3.09*100.65/100
3.09*101/100
3.09*102/100
93.13-91.76
1.37/93.13
5.2-5.01
0.19/5
0.19/5.2
library(sparklyr)
install.packages('sparklyr')
library(sparklyr)
sc <- spark_connect(master = "local")
spark_install()
sc <- spark_connect(master = "local")
library(dplyr)
iris_tbl <- copy_to(sc, iris)
iris_tbl
spark_apply(iris_tbl, function(data) {
data[1:4] + rgamma(1,2)
})
library(sparklyr)
sc <- spark_connect(master = "local")
library(dplyr)
iris_tbl <- copy_to(sc, iris)
iris_tbl
spark_apply(iris_tbl, function(data) {
data[1:4] + rgamma(1,2)
})
spark_apply(
iris_tbl,
function(e) broom::tidy(lm(Petal_Width ~ Petal_Length, e)),
names = c("term", "estimate", "std.error", "statistic", "p.value"),
group_by = "Species"
)
library(ggplot2)
library(ggmap)
register_google(key = "[AIzaSyCl8BVE2jB-EAUtzvjrUonBoKm-Rz8VdBg]")
#point
north_ca = get_map(location = c(lon=mean(quakes$long), lat=mean(quakes$lat)),
maptype="terrain", zoom=4)
Sys.setenv(http_proxy="http://KEM.SG7.SSR.LC:10173")
library(ggplot2)
library(ggmap)
register_google(key = "[AIzaSyCl8BVE2jB-EAUtzvjrUonBoKm-Rz8VdBg]")
#point
north_ca = get_map(location = c(lon=mean(quakes$long), lat=mean(quakes$lat)),
maptype="terrain", zoom=4)
Sys.getenv("http_proxy")
Sys.getenv("http_proxy_user")
Sys.getenv("https_proxy")
Sys.setenv(http_proxy="http://KEM.SG7.SSR.LC:10173")
library(ggplot2)
library(ggmap)
register_google(key = "[AIzaSyCl8BVE2jB-EAUtzvjrUonBoKm-Rz8VdBg]")
#point
north_ca = get_map(location = c(lon=mean(quakes$long), lat=mean(quakes$lat)),
maptype="terrain", zoom=4)
a=ggmap(north_ca) +
geom_point(data=quakes, aes(x=long, y=lat, colour=depth), alpha=0.8) +
labs(x="", y="") +
scale_colour_gradient(low="#0080FF", high="tomato")+
labs(x = "Longitude", y = "Latitude")+
ggtitle("Earthquakes in Northern California 2008 - 2011 YangJichen")
setwd('/Users/yangjichen/Desktop/课程/数据可视化/HW3')
ggsave('problem4a.jpg', a)
################################
Sys.setenv(http_proxy="http://KEM.SG7.SSR.LC:10173")
library(ggplot2)
library(ggmap)
register_google(key = "[AIzaSyCl8BVE2jB-EAUtzvjrUonBoKm-Rz8VdBg]")
#point
north_ca = get_map(location = c(lon=mean(quakes$long), lat=mean(quakes$lat)),
maptype="terrain", zoom=4)
Sys.setenv(http_proxy="http://KEM.SG7.SSR.LC:10173")
library(ggplot2)
library(ggmap)
register_google(key = "[AIzaSyCl8BVE2jB-EAUtzvjrUonBoKm-Rz8VdBg]")
#point
north_ca = get_map(location = c(lon=mean(quakes$long), lat=mean(quakes$lat)),
maptype="terrain", zoom=4)
Sys.setenv(http_proxy="sf1.kingss.me:22722")
library(ggplot2)
library(ggmap)
register_google(key = "[AIzaSyCl8BVE2jB-EAUtzvjrUonBoKm-Rz8VdBg]")
#point
north_ca = get_map(location = c(lon=mean(quakes$long), lat=mean(quakes$lat)),
maptype="terrain", zoom=4)
register_google(key = "AIzaSyCl8BVE2jB-EAUtzvjrUonBoKm-Rz8VdBg")
#point
north_ca = get_map(location = c(lon=mean(quakes$long), lat=mean(quakes$lat)),
maptype="terrain", zoom=4)
register_google(key = "AIzaSyCl8BVE2jB-EAUtzvjrUonBoKm-Rz8VdBg")
#point
north_ca = get_map(location = c(lon=mean(quakes$long), lat=mean(quakes$lat)),
maptype="terrain", zoom=4)
library(ggplot2)
library(ggmap)
register_google(key = "AIzaSyCl8BVE2jB-EAUtzvjrUonBoKm-Rz8VdBg")
#point
north_ca = get_map(location = c(lon=mean(quakes$long), lat=mean(quakes$lat)),
maptype="terrain", zoom=4)
library(ggplot2)
library(ggmap)
register_google(key = "AIzaSyCl8BVE2jB-EAUtzvjrUonBoKm-Rz8VdBg")
#point
north_ca = get_map(location = c(lon=mean(quakes$long), lat=mean(quakes$lat)),
maptype="terrain", zoom=4)
register_google(key = "[AIzaSyCl8BVE2jB-EAUtzvjrUonBoKm-Rz8VdBg]")
#point
north_ca = get_map(location = c(lon=mean(quakes$long), lat=mean(quakes$lat)),
maptype="terrain", zoom=4)
register_google(key = "AIzaSyCl8BVE2jB-EAUtzvjrUonBoKm-Rz8VdBg")
#point
north_ca = get_map(location = c(lon=mean(quakes$long), lat=mean(quakes$lat)),
maptype="terrain", zoom=4)
library(ggplot2)
library(ggmap)
register_google(key = "AIzaSyCl8BVE2jB-EAUtzvjrUonBoKm-Rz8VdBg")
#point
north_ca = get_map(location = c(lon=mean(quakes$long), lat=mean(quakes$lat)),
maptype="terrain", zoom=4)
#point
north_ca = get_map(location = c(lon=mean(quakes$long), lat=mean(quakes$lat)),
maptype="terrain", zoom=4)
showing_key()
ggmap_show_api_key()
google_key()
#point
north_ca = get_map(location = c(lon=mean(quakes$long), lat=mean(quakes$lat)),
maptype="terrain", zoom=4)
#point
north_ca = get_map(location = c(lon=mean(quakes$long), lat=mean(quakes$lat)),
maptype="terrain", zoom=4)
register_google(key = "AIzaSyCl8BVE2jB-EAUtzvjrUonBoKm-Rz8VdBg")
#point
north_ca = get_map(location = c(lon=mean(quakes$long), lat=mean(quakes$lat)),
maptype="terrain", zoom=4)
ggmap_show_api_key()
register_google(key = "AIzaSyCl8BVE2jB-EAUtzvjrUonBoKm-Rz8VdBg")
#point
north_ca = get_map(location = c(lon=mean(quakes$long), lat=mean(quakes$lat)),
maptype="terrain", zoom=4)
?get_googlemap
#point
north_ca = get_googlemap(location = c(lon=mean(quakes$long), lat=mean(quakes$lat)),
maptype="terrain", zoom=4)
quakes
#point
north_ca = get_map(location = c(lon=mean(quakes$long), lat=mean(quakes$lat)),
maptype="terrain", zoom=4)
R.home()
get_map
library(httr)
set_config(use_proxy(url="127.0.0.1",port=1080))
library(ggplot2)
library(ggmap)
register_google(key = "AIzaSyCl8BVE2jB-EAUtzvjrUonBoKm-Rz8VdBg")
#point
north_ca = get_map(location = c(lon=mean(quakes$long), lat=mean(quakes$lat)),
maptype="terrain", zoom=4)
set_config(use_proxy(url="127.0.0.1",port=8090))
library(ggplot2)
library(ggmap)
register_google(key = "AIzaSyCl8BVE2jB-EAUtzvjrUonBoKm-Rz8VdBg")
#point
north_ca = get_map(location = c(lon=mean(quakes$long), lat=mean(quakes$lat)),
maptype="terrain", zoom=4)
set_config(use_proxy(url="127.0.0.1",port=8090))
library(ggplot2)
library(ggmap)
register_google(key = "AIzaSyCl8BVE2jB-EAUtzvjrUonBoKm-Rz8VdBg")
#point
north_ca = get_map(location = c(lon=mean(quakes$long), lat=mean(quakes$lat)),
maptype="terrain", zoom=4)
library(httr)
set_config(use_proxy(url="127.0.0.1",port=1087))
library(ggplot2)
library(ggmap)
register_google(key = "AIzaSyCl8BVE2jB-EAUtzvjrUonBoKm-Rz8VdBg")
#point
north_ca = get_map(location = c(lon=mean(quakes$long), lat=mean(quakes$lat)),
maptype="terrain", zoom=4)
a=ggmap(north_ca) +
geom_point(data=quakes, aes(x=long, y=lat, colour=depth), alpha=0.8) +
labs(x="", y="") +
scale_colour_gradient(low="#0080FF", high="tomato")+
labs(x = "Longitude", y = "Latitude")+
ggtitle("Earthquakes in Northern California 2008 - 2011 YangJichen")
a
data(baseball, package="plyr")
baseball %>%
group_by(id) %>%
summarise(
seasons = max(year)-min(year)+1, atbats = sum(ab),
avg = sum(h, na.rm=T)/sum(ab, na.rm=T)
)
baseball %.%
group_by(id) %.%
summarise(
seasons = max(year)-min(year)+1, atbats = sum(ab),
avg = sum(h, na.rm=T)/sum(ab, na.rm=T)
)
data(baseball, package="plyr")
baseball %>%
group_by(id) %>%
summarise(
seasons = max(year)-min(year)+1, atbats = sum(ab),
avg = sum(h, na.rm=T)/sum(ab, na.rm=T)
)
library('sparklyr')
sc = spark_connect(master = 'local')
library('dplyr')
head('iris')
iris_tbl <- sdf_copy_to(sc = sc, x = iris, overwrite = T)
sc = spark_connect(master = 'local')
sc = spark_connect(master = 'local')
library('dplyr')
head('iris')
iris_tbl <- sdf_copy_to(sc = sc, x = iris, overwrite = T)
library('sparklyr')
library('dplyr')
sc = spark_connect(master = 'local')
iris_tbl <- sdf_copy_to(sc = sc, x = iris, overwrite = T)
data(baseball, package="plyr")
baseball %>%
group_by(id) %>%
summarise(
seasons = max(year)-min(year)+1, atbats = sum(ab),
avg = sum(h, na.rm=T)/sum(ab, na.rm=T)
)
library(dplyr)
library(sparklyr)
library(ggplot2)
baseball = plyr::baseball
sc <- spark_connect(master = "local")
baseball_tbl = copy_to(sc, baseball)
library(dplyr)
library(sparklyr)
library(ggplot2)
baseball = plyr::baseball
sc <- spark_connect(master = "local")
data(baseball, package="plyr")
baseball_tbl %>%
group_by(id) %>%
summarise(
seasons = max(year)-min(year)+1, atbats = sum(ab),
avg = sum(h, na.rm=T)/sum(ab, na.rm=T)
)
baseball_tbl = copy_to(sc, baseball)
baseball_tbl %>%
group_by(id) %>%
summarise(
seasons = max(year)-min(year)+1, atbats = sum(ab),
avg = sum(h, na.rm=T)/sum(ab, na.rm=T)
)
baseball_tbl
library('ggplot2')
library(dplyr)
library(sparklyr)
sc <- spark_connect(master = "local")
diamonds_tbl = copy_to(sc, diamonds)
help(diamonds)
summary(diamonds)
model_tbl = diamonds_tbl %>%
filter(!is.na(price)&!is.na(carat)&!is.na(cut)&!is.na(color)&!is.na(clarity)) %>%
mutate(price_rank = as.numeric(price > 5324),
log_price = log10(price),
new_carat = carat^(1/3))
library(gss)
?gssanova
e^12.4
exp(12.4/100*5)
sqrt(log(5)*4)*2
log(5)
log(5)*4
sqrt(6)
sqrt(log(5)*2)*2
(0.29676+0.31876+0.33337+0.30549)/4
#the standard BVN density f <- function(x,y) {
z <- (1/(2*pi)) * exp(-.5 * (x^2 + y^2)) }
y <- x <- seq(-3, 3, length= 50)
z <- outer(x, y, f) #compute density for all (x,y)
persp(x, y, z) #the default plot
persp(x, y, z, theta = 45, phi = 30, expand = 0.6, ltheta = 120,
shade = 0.75, ticktype = "detailed", xlab = "X", ylab = "Y", zlab = "f(x, y)")
#the standard BVN density
f <- function(x,y) {
z <- (1/(2*pi)) * exp(-.5 * (x^2 + y^2)) }
y <- x <- seq(-3, 3, length= 50)
z <- outer(x, y, f) #compute density for all (x,y)
persp(x, y, z) #the default plot
persp(x, y, z, theta = 45, phi = 30, expand = 0.6, ltheta = 120,
shade = 0.75, ticktype = "detailed", xlab = "X", ylab = "Y", zlab = "f(x, y)")
persp(x, y, z) #the default plot
persp(x, y, z, theta = 45, phi = 30, expand = 0.6, ltheta = 120,
shade = 0.75, ticktype = "detailed", xlab = "X", ylab = "Y", zlab = "f(x, y)")
?persp
par(bg = "white")
x <- seq(-1.95, 1.95, length = 30)
y <- seq(-1.95, 1.95, length = 35)
z <- outer(x, y, function(a, b) a*b^2)
nrz <- nrow(z)
ncz <- ncol(z)
# Create a function interpolating colors in the range of specified colors
jet.colors <- colorRampPalette( c("blue", "green") )
# Generate the desired number of colors from this palette
nbcol <- 100
color <- jet.colors(nbcol)
# Compute the z-value at the facet centres
zfacet <- z[-1, -1] + z[-1, -ncz] + z[-nrz, -1] + z[-nrz, -ncz]
# Recode facet z-values into color indices
facetcol <- cut(zfacet, nbcol)
persp(x, y, z, col = color[facetcol], phi = 30, theta = -30)
par(op)
x <- seq(-1.95, 1.95, length = 30)
y <- seq(-1.95, 1.95, length = 35)
z <- outer(x, y, function(a, b) a*b^2)
nrz <- nrow(z)
ncz <- ncol(z)
# Create a function interpolating colors in the range of specified colors
jet.colors <- colorRampPalette( c("blue", "green") )
# Generate the desired number of colors from this palette
nbcol <- 100
color <- jet.colors(nbcol)
# Compute the z-value at the facet centres
zfacet <- z[-1, -1] + z[-1, -ncz] + z[-nrz, -1] + z[-nrz, -ncz]
# Recode facet z-values into color indices
facetcol <- cut(zfacet, nbcol)
persp(x, y, z, col = color[facetcol], phi = 30, theta = -30)
z <- 2 * volcano        # Exaggerate the relief
x <- 10 * (1:nrow(z))   # 10 meter spacing (S to N)
y <- 10 * (1:ncol(z))   # 10 meter spacing (E to W)
## Don't draw the grid lines :  border = NA
par(bg = "slategray")
persp(x, y, z, theta = 135, phi = 30, col = "green3", scale = FALSE,
ltheta = -120, shade = 0.75, border = NA, box = FALSE)
xE <- c(-10,10); xy <- expand.grid(xE, xE)
points(trans3d(xy[,1], xy[,2], 6, pmat = res), col = 2, pch = 16)
lines (trans3d(x, y = 10, z = 6 + sin(x), pmat = res), col = 3)
phi <- seq(0, 2*pi, len = 201)
r1 <- 7.725 # radius of 2nd maximum
xr <- r1 * cos(phi)
yr <- r1 * sin(phi)
lines(trans3d(xr,yr, f(xr,yr), res), col = "pink", lwd = 2)
x <- seq(-10, 10, length= 30)
y <- x
f <- function(x, y) { r <- sqrt(x^2+y^2); 10 * sin(r)/r }
z <- outer(x, y, f)
z[is.na(z)] <- 1
op <- par(bg = "white")
persp(x, y, z, theta = 30, phi = 30, expand = 0.5, col = "lightblue")
persp(x, y, z, theta = 30, phi = 30, expand = 0.5, col = "lightblue",
ltheta = 120, shade = 0.75, ticktype = "detailed",
xlab = "X", ylab = "Y", zlab = "Sinc( r )"
) -> res
round(res, 3)
setwd('/Users/yangjichen/Desktop/project/Pub_opi_tensor/exponential\ family\ tensor/code/realdata')
library(RcppCNPy)
a = npyLoad('GDELTstep5.npy', type="numeric")
x <- c(0.001,0.01,0.1,1,10)
y <- c(0.001,0.01,0.1,1,10)
z <- outer(x, y, function(a, b) a*b^2)
persp(x, y, z, theta = 45, phi = 30, expand = 0.6, ltheta = 120,
shade = 0.75, ticktype = "detailed", xlab = "X", ylab = "Y", zlab = "f(x, y)")
persp(x, y, z, theta = 45, phi = 30, expand = 0.6, ltheta = 120,
shade = 0.75, ticktype = "detailed", xlab = "X", ylab = "Y", zlab = "f(x, y)",log='x')
x <- 1:100
y <- 1:100
z <- outer(x,y, function(x, y) exp(x/10 + y/10)
logz <- log(z, 10)
persp3d(x,y,logz, axes=FALSE, zlab="z", col="red")
axes3d(c("x", "y"))
axis3d("z", at=pretty(logz), labels=10^pretty(logz))
